{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da00dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "# from activators import SigmoidActivator\n",
    "%run activators.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6477877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全连接层实现类\n",
    "class FullConnectedLayer(object):\n",
    "    def __init__(self, input_size, output_size,\n",
    "                 activator):\n",
    "        '''\n",
    "        构造函数\n",
    "        input_size: 本层输入向量的维度\n",
    "        output_size: 本层输出向量的维度\n",
    "        activator: 激活函数\n",
    "        '''\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activator = activator\n",
    "        # 权重数组W\n",
    "        self.W = np.random.uniform(-0.1, 0.1,\n",
    "                                   (output_size, input_size))\n",
    "        # 偏置项b\n",
    "        self.b = np.zeros((output_size, 1))\n",
    "        # 输出向量\n",
    "        self.output = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, input_array):\n",
    "        '''\n",
    "        前向计算\n",
    "        input_array: 输入向量，维度必须等于input_size\n",
    "        '''\n",
    "        # 式2\n",
    "        self.input = input_array\n",
    "        self.output = self.activator.forward(\n",
    "            np.dot(self.W, input_array) + self.b)\n",
    "\n",
    "    def backward(self, delta_array):\n",
    "        '''\n",
    "        反向计算W和b的梯度\n",
    "        delta_array: 从上一层传递过来的误差项\n",
    "        '''\n",
    "        # 式8\n",
    "        self.delta = self.activator.backward(self.input) * np.dot(\n",
    "            self.W.T, delta_array)\n",
    "        self.W_grad = np.dot(delta_array, self.input.T)\n",
    "        self.b_grad = delta_array\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        '''\n",
    "        使用梯度下降算法更新权重\n",
    "        '''\n",
    "        self.W += learning_rate * self.W_grad\n",
    "        self.b += learning_rate * self.b_grad\n",
    "\n",
    "    def dump(self):\n",
    "        print('W: %s\\nb:%s' % (self.W, self.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络类\n",
    "class Network(object):\n",
    "    def __init__(self, layers):\n",
    "        '''\n",
    "        构造函数\n",
    "        '''\n",
    "        self.layers = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(\n",
    "                FullConnectedLayer(\n",
    "                    layers[i], layers[i + 1],\n",
    "                    SigmoidActivator()\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def predict(self, sample):\n",
    "        '''\n",
    "        使用神经网络实现预测\n",
    "        sample: 输入样本\n",
    "        '''\n",
    "        output = sample\n",
    "        for layer in self.layers:\n",
    "            layer.forward(output)\n",
    "            output = layer.output\n",
    "        return output\n",
    "    \n",
    "    def train(self, labels, data_set, rate, epoch):\n",
    "        '''\n",
    "        训练函数\n",
    "        labels: 样本标签\n",
    "        data_set: 输入样本\n",
    "        rate: 学习速率\n",
    "        epoch: 训练轮数\n",
    "        '''\n",
    "        for i in range(epoch):\n",
    "            for d in range(len(data_set)):\n",
    "                self.train_one_sample(labels[d],\n",
    "                                      data_set[d], rate)\n",
    "\n",
    "    def train_one_sample(self, label, sample, rate):\n",
    "        self.predict(sample)\n",
    "        self.calc_gradient(label)\n",
    "        self.update_weight(rate)\n",
    "\n",
    "    def calc_gradient(self, label):\n",
    "        delta = self.layers[-1].activator.backward(\n",
    "            self.layers[-1].output\n",
    "        ) * (label - self.layers[-1].output)\n",
    "        for layer in self.layers[::-1]:\n",
    "            layer.backward(delta)\n",
    "            delta = layer.delta\n",
    "        return delta\n",
    "\n",
    "    def update_weight(self, rate):\n",
    "        for layer in self.layers:\n",
    "            layer.update(rate)\n",
    "            \n",
    "    def dump(self):\n",
    "        for layer in self.layers:\n",
    "            layer.dump()\n",
    "\n",
    "    def loss(self, output, label):\n",
    "        return 0.5 * ((label - output) * (label - output)).sum()\n",
    "\n",
    "    def gradient_check(self, sample_feature, sample_label):\n",
    "        '''\n",
    "        梯度检查\n",
    "        network: 神经网络对象\n",
    "        sample_feature: 样本的特征\n",
    "        sample_label: 样本的标签\n",
    "        '''\n",
    "\n",
    "        # 获取网络在当前样本下每个连接的梯度\n",
    "        self.predict(sample_feature)\n",
    "        self.calc_gradient(sample_label)\n",
    "\n",
    "        # 检查梯度\n",
    "        epsilon = 10e-4\n",
    "        for fc in self.layers:\n",
    "            for i in range(fc.W.shape[0]):\n",
    "                for j in range(fc.W.shape[1]):\n",
    "                    fc.W[i, j] += epsilon\n",
    "                    output = self.predict(sample_feature)\n",
    "                    err1 = self.loss(sample_label, output)\n",
    "                    fc.W[i, j] -= 2 * epsilon\n",
    "                    output = self.predict(sample_feature)\n",
    "                    err2 = self.loss(sample_label, output)\n",
    "                    expect_grad = (err1 - err2) / (2 * epsilon)\n",
    "                    fc.W[i, j] += epsilon\n",
    "                    print('weights(%d,%d): expected - actural %.4e - %.4e' % (\n",
    "                        i, j, expect_grad, fc.W_grad[i, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac5f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bp import train_data_set\n",
    "def transpose(args):\n",
    "    return list(map(\n",
    "        lambda arg: list(map(\n",
    "            lambda line: np.array(line).reshape(len(line), 1)\n",
    "            , arg))\n",
    "        , args\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4caae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(object):\n",
    "    def __init__(self):\n",
    "        self.mask = [\n",
    "            0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, 0x80\n",
    "        ]\n",
    "\n",
    "    def norm(self, number):\n",
    "        data = list(map(lambda m: 0.9 if number & m else 0.1, self.mask))\n",
    "        return np.array(data).reshape(8, 1)\n",
    "\n",
    "    def denorm(self, vec):\n",
    "        binary = list(map(lambda i: 1 if i > 0.5 else 0, vec[:, 0]))\n",
    "        for i in range(len(self.mask)):\n",
    "            binary[i] = binary[i] * self.mask[i]\n",
    "        return reduce(lambda x, y: x + y, binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_set():\n",
    "    normalizer = Normalizer()\n",
    "    data_set = []\n",
    "    labels = []\n",
    "    for i in range(0, 256):\n",
    "        n = normalizer.norm(i)\n",
    "        data_set.append(n)\n",
    "        labels.append(n)\n",
    "    return labels, data_set\n",
    "\n",
    "\n",
    "def correct_ratio(network):\n",
    "    normalizer = Normalizer()\n",
    "    correct = 0.0;\n",
    "    for i in range(256):\n",
    "        if normalizer.denorm(network.predict(normalizer.norm(i))) == i:\n",
    "            correct += 1.0\n",
    "    print('correct_ratio: %.2f%%' % (correct / 256 * 100))\n",
    "\n",
    "\n",
    "def test():\n",
    "    labels, data_set = transpose(train_data_set())\n",
    "    net = Network([8, 3, 8])\n",
    "    rate = 0.5\n",
    "    mini_batch = 20\n",
    "    epoch = 10\n",
    "    for i in range(epoch):\n",
    "        net.train(labels, data_set, rate, mini_batch)\n",
    "        print('after epoch %d loss: %f' % (\n",
    "            (i + 1),\n",
    "            net.loss(labels[-1], net.predict(data_set[-1]))\n",
    "        ))\n",
    "        rate /= 2\n",
    "    correct_ratio(net)\n",
    "\n",
    "\n",
    "def gradient_check():\n",
    "    '''\n",
    "    梯度检查\n",
    "    '''\n",
    "    labels, data_set = transpose(train_data_set())\n",
    "    net = Network([8, 3, 8])\n",
    "    net.gradient_check(data_set[0], labels[0])\n",
    "    return net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
