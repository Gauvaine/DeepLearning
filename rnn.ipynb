{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd419841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 6.  7.  5.]\n",
      "  [ 3. -1. -1.]\n",
      "  [ 2. -1.  4.]]\n",
      "\n",
      " [[ 2. -5. -8.]\n",
      "  [ 1. -4. -4.]\n",
      "  [ 0. -5. -5.]]]\n",
      "filter weights:\n",
      "array([[[-1.008,  0.99 , -0.009],\n",
      "        [-0.005,  0.994, -0.006],\n",
      "        [-0.006,  0.995,  0.996]],\n",
      "\n",
      "       [[-1.004, -1.001, -0.004],\n",
      "        [-0.01 , -0.009, -0.012],\n",
      "        [-0.002, -1.002, -0.002]],\n",
      "\n",
      "       [[-0.002, -0.002, -1.003],\n",
      "        [-0.005,  0.992, -0.005],\n",
      "        [ 0.993, -1.008, -1.007]]])\n",
      "bias:\n",
      "0.991\n",
      "filter weights:\n",
      "array([[[ 9.980e-01,  9.980e-01, -1.001e+00],\n",
      "        [-1.004e+00, -1.007e+00,  9.970e-01],\n",
      "        [-4.000e-03, -1.004e+00,  9.980e-01]],\n",
      "\n",
      "       [[ 0.000e+00,  9.990e-01,  0.000e+00],\n",
      "        [-1.009e+00, -5.000e-03, -1.004e+00],\n",
      "        [-1.004e+00,  1.000e+00,  0.000e+00]],\n",
      "\n",
      "       [[-1.004e+00, -6.000e-03, -5.000e-03],\n",
      "        [-1.002e+00, -5.000e-03,  9.980e-01],\n",
      "        [-1.002e+00, -1.000e-03,  0.000e+00]]])\n",
      "bias:\n",
      "-0.007\n",
      "weights(0,0,0): expected - actural 5.000000 - 5.000000\n",
      "weights(0,0,1): expected - actural 6.000000 - 6.000000\n",
      "weights(0,0,2): expected - actural 5.000000 - 5.000000\n",
      "weights(0,1,0): expected - actural 5.000000 - 5.000000\n",
      "weights(0,1,1): expected - actural 7.000000 - 7.000000\n",
      "weights(0,1,2): expected - actural 5.000000 - 5.000000\n",
      "weights(0,2,0): expected - actural 5.000000 - 5.000000\n",
      "weights(0,2,1): expected - actural 6.000000 - 6.000000\n",
      "weights(0,2,2): expected - actural 5.000000 - 5.000000\n",
      "weights(1,0,0): expected - actural 2.000000 - 2.000000\n",
      "weights(1,0,1): expected - actural 1.000000 - 1.000000\n",
      "weights(1,0,2): expected - actural 2.000000 - 2.000000\n",
      "weights(1,1,0): expected - actural 9.000000 - 9.000000\n",
      "weights(1,1,1): expected - actural 9.000000 - 9.000000\n",
      "weights(1,1,2): expected - actural 9.000000 - 9.000000\n",
      "weights(1,2,0): expected - actural 2.000000 - 2.000000\n",
      "weights(1,2,1): expected - actural 1.000000 - 1.000000\n",
      "weights(1,2,2): expected - actural 2.000000 - 2.000000\n",
      "weights(2,0,0): expected - actural 4.000000 - 4.000000\n",
      "weights(2,0,1): expected - actural 5.000000 - 5.000000\n",
      "weights(2,0,2): expected - actural 4.000000 - 4.000000\n",
      "weights(2,1,0): expected - actural 4.000000 - 4.000000\n",
      "weights(2,1,1): expected - actural 9.000000 - 9.000000\n",
      "weights(2,1,2): expected - actural 4.000000 - 4.000000\n",
      "weights(2,2,0): expected - actural 4.000000 - 4.000000\n",
      "weights(2,2,1): expected - actural 5.000000 - 5.000000\n",
      "weights(2,2,2): expected - actural 4.000000 - 4.000000\n",
      "input array:\n",
      "[[[1. 1. 2. 4.]\n",
      "  [5. 6. 7. 8.]\n",
      "  [3. 2. 1. 0.]\n",
      "  [1. 2. 3. 4.]]\n",
      "\n",
      " [[0. 1. 2. 3.]\n",
      "  [4. 5. 6. 7.]\n",
      "  [8. 9. 0. 1.]\n",
      "  [3. 4. 5. 6.]]]\n",
      "output array:\n",
      "[[[6. 8.]\n",
      "  [3. 4.]]\n",
      "\n",
      " [[5. 7.]\n",
      "  [9. 6.]]]\n",
      "input array:\n",
      "[[[1. 1. 2. 4.]\n",
      "  [5. 6. 7. 8.]\n",
      "  [3. 2. 1. 0.]\n",
      "  [1. 2. 3. 4.]]\n",
      "\n",
      " [[0. 1. 2. 3.]\n",
      "  [4. 5. 6. 7.]\n",
      "  [8. 9. 0. 1.]\n",
      "  [3. 4. 5. 6.]]]\n",
      "sensitivity array:\n",
      "[[[1. 2.]\n",
      "  [2. 4.]]\n",
      "\n",
      " [[3. 5.]\n",
      "  [8. 2.]]]\n",
      "delta array:\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 1. 0. 2.]\n",
      "  [2. 0. 0. 0.]\n",
      "  [0. 0. 0. 4.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 3. 0. 5.]\n",
      "  [0. 8. 0. 0.]\n",
      "  [0. 0. 0. 2.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "%run activators.ipynb\n",
    "%run cnn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "456acb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentLayer(object):\n",
    "    # state_list的行数等于时间步的数量，对于每个state行数通常对应隐藏层神经元的数量，而列数则对应内部状态如隐藏状态等\n",
    "    # state_width指隐藏层神经元的个数\n",
    "    # delta_list同state_list\n",
    "    # times指时间步长的数量\n",
    "    def __init__(self,input_width,state_width,activator,learning_rate):\n",
    "        self.input_width = input_width\n",
    "        self.state_width = state_width\n",
    "        self.activator = activator\n",
    "        self.learning_rate = learning_rate\n",
    "        self.times = 0       # 当前时刻初始化为t0\n",
    "        self.state_list = [] # 保存各个时刻的state\n",
    "        self.state_list.append(np.zeros((state_width, 1))) #初始化s0\n",
    "        self.U = np.random.uniform(-1e-4, 1e-4,(state_width, input_width))  # 初始化U\n",
    "        self.W = np.random.uniform(-1e-4, 1e-4,(state_width, state_width))  # 初始化W\n",
    "        \n",
    "    # 前向计算\n",
    "    def forward(self,input_array):\n",
    "        self.times += 1\n",
    "        state = (np.dot(self.U, input_array) +np.dot(self.W, self.state_list[-1]))\n",
    "        element_wise_op(state, self.activator.forward)\n",
    "        self.state_list.append(state)\n",
    "        \n",
    "    # 实现BPTT算法\n",
    "    def backward(self, sensitivity_array, activator):\n",
    "        self.calc_delta(sensitivity_array, activator)\n",
    "        self.calc_gradient()\n",
    "    \n",
    "    # 按照梯度下降，更新权重\n",
    "    def update(self):\n",
    "        self.W -= self.learning_rate * self.gradient\n",
    "        \n",
    "    def calc_delta(self,sensitivity_array,activator):\n",
    "        # sensitivity_array:输出层的误差项\n",
    "        self.delta_list=[] # 保存各个时刻的误差项\n",
    "        for i in range(self.times):\n",
    "            self.delta_list.append(np.zeros((self.state_width,1)))\n",
    "        self.delta_list.append(sensitivity_array)\n",
    "        # 迭代计算每个时刻的误差项\n",
    "        for k in range(self.times-1,0,-1):\n",
    "            self.calc_delta_k(k,activator)\n",
    "            \n",
    "    def calc_delta_k(self,k,activator):\n",
    "        # .copy():浅拷贝，确保state变量始终包含原始、未更新的隐藏状态值\n",
    "        # 如果不使用.copy()方法，而是直接引用self.state_list[k+1]，那么在权重更新后，这个引用也会指向更新后的值，导致我们的delta计算基于错误的数据\n",
    "        state = self.state_list[k+1].copy()\n",
    "        element_wise_op(self.state_list[k+1],activator.backward)\n",
    "        # state[:, 0] 的意思是选取state数组中所有行的第0列的元素，即从state数组中提取了所有行（即所有神经元）的第一列（即隐藏状态）\n",
    "        self.delta_list[k]=np.dot(np.dot(self.delta_list[k+1].T,self.W),np.diag(state[:,0])).T\n",
    "        \n",
    "    def calc_gradient(self):\n",
    "        self.gradient_list = [] # 保存各个时刻的权重梯度\n",
    "        for t in range(self.times + 1):\n",
    "            self.gradient_list.append(np.zeros((self.state_width, self.state_width)))\n",
    "        for t in range(self.times, 0, -1):\n",
    "            self.calc_gradient_t(t)\n",
    "        # 实际的梯度是各个时刻梯度之和\n",
    "        self.gradient = reduce(lambda a, b: a + b, self.gradient_list,self.gradient_list[0]) # [0]被初始化为0且没有被修改过\n",
    "        \n",
    "    # 计算每个时刻t权重的梯度\n",
    "    def calc_gradient_t(self, t):\n",
    "        gradient = np.dot(self.delta_list[t],self.state_list[t-1].T)\n",
    "        self.gradient_list[t] = gradient\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.times = 0       # 当前时刻初始化为t0\n",
    "        self.state_list = [] # 保存各个时刻的state\n",
    "        self.state_list.append(np.zeros((self.state_width, 1)))      # 初始化s0\n",
    "\n",
    "def data_set():\n",
    "    x = [np.array([[1], [2], [3]]),\n",
    "         np.array([[2], [3], [4]])]\n",
    "    d = np.array([[1], [2]])\n",
    "    return x, d\n",
    "\n",
    "# 梯度检查\n",
    "def gradient_check():\n",
    "    # 设计一个误差函数，取所有节点输出项之和\n",
    "    error_function = lambda o: o.sum()\n",
    "    \n",
    "    rl = RecurrentLayer(3, 2, IdentityActivator(), 1e-3)\n",
    "\n",
    "    # 计算forward值\n",
    "    x, d = data_set()\n",
    "    rl.forward(x[0])\n",
    "    rl.forward(x[1])\n",
    "    \n",
    "    # 求取sensitivity map\n",
    "    sensitivity_array = np.ones(rl.state_list[-1].shape,dtype=np.float64)\n",
    "    # 计算梯度\n",
    "    rl.backward(sensitivity_array, IdentityActivator())\n",
    "    \n",
    "    # 检查梯度\n",
    "    epsilon = 10e-4\n",
    "    for i in range(rl.W.shape[0]):\n",
    "        for j in range(rl.W.shape[1]):\n",
    "            rl.W[i,j] += epsilon\n",
    "            rl.reset_state()\n",
    "            rl.forward(x[0])\n",
    "            rl.forward(x[1])\n",
    "            err1 = error_function(rl.state_list[-1])\n",
    "            rl.W[i,j] -= 2*epsilon\n",
    "            rl.reset_state()\n",
    "            rl.forward(x[0])\n",
    "            rl.forward(x[1])\n",
    "            err2 = error_function(rl.state_list[-1])\n",
    "            expect_grad = (err1 - err2) / (2 * epsilon)\n",
    "            rl.W[i,j] += epsilon\n",
    "            print('weights(%d,%d): expected - actural %f - %f' % (i, j, expect_grad, rl.gradient[i,j]))\n",
    "\n",
    "def test():\n",
    "    l = RecurrentLayer(3, 2, ReluActivator(), 1e-3)\n",
    "    x, d = data_set()\n",
    "    l.forward(x[0])\n",
    "    l.forward(x[1])\n",
    "    l.backward(d, ReluActivator())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "649f1460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights(0,0): expected - actural -0.000175 - -0.000175\n",
      "weights(0,1): expected - actural 0.000218 - 0.000218\n",
      "weights(1,0): expected - actural -0.000175 - -0.000175\n",
      "weights(1,1): expected - actural 0.000218 - 0.000218\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test()\n",
    "    gradient_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1055495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
