{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b351bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 6.  7.  5.]\n",
      "  [ 3. -1. -1.]\n",
      "  [ 2. -1.  4.]]\n",
      "\n",
      " [[ 2. -5. -8.]\n",
      "  [ 1. -4. -4.]\n",
      "  [ 0. -5. -5.]]]\n",
      "filter weights:\n",
      "array([[[-1.008,  0.99 , -0.009],\n",
      "        [-0.005,  0.994, -0.006],\n",
      "        [-0.006,  0.995,  0.996]],\n",
      "\n",
      "       [[-1.004, -1.001, -0.004],\n",
      "        [-0.01 , -0.009, -0.012],\n",
      "        [-0.002, -1.002, -0.002]],\n",
      "\n",
      "       [[-0.002, -0.002, -1.003],\n",
      "        [-0.005,  0.992, -0.005],\n",
      "        [ 0.993, -1.008, -1.007]]])\n",
      "bias:\n",
      "0.991\n",
      "filter weights:\n",
      "array([[[ 9.980e-01,  9.980e-01, -1.001e+00],\n",
      "        [-1.004e+00, -1.007e+00,  9.970e-01],\n",
      "        [-4.000e-03, -1.004e+00,  9.980e-01]],\n",
      "\n",
      "       [[ 0.000e+00,  9.990e-01,  0.000e+00],\n",
      "        [-1.009e+00, -5.000e-03, -1.004e+00],\n",
      "        [-1.004e+00,  1.000e+00,  0.000e+00]],\n",
      "\n",
      "       [[-1.004e+00, -6.000e-03, -5.000e-03],\n",
      "        [-1.002e+00, -5.000e-03,  9.980e-01],\n",
      "        [-1.002e+00, -1.000e-03,  0.000e+00]]])\n",
      "bias:\n",
      "-0.007\n",
      "weights(0,0,0): expected - actural 5.000000 - 5.000000\n",
      "weights(0,0,1): expected - actural 6.000000 - 6.000000\n",
      "weights(0,0,2): expected - actural 5.000000 - 5.000000\n",
      "weights(0,1,0): expected - actural 5.000000 - 5.000000\n",
      "weights(0,1,1): expected - actural 7.000000 - 7.000000\n",
      "weights(0,1,2): expected - actural 5.000000 - 5.000000\n",
      "weights(0,2,0): expected - actural 5.000000 - 5.000000\n",
      "weights(0,2,1): expected - actural 6.000000 - 6.000000\n",
      "weights(0,2,2): expected - actural 5.000000 - 5.000000\n",
      "weights(1,0,0): expected - actural 2.000000 - 2.000000\n",
      "weights(1,0,1): expected - actural 1.000000 - 1.000000\n",
      "weights(1,0,2): expected - actural 2.000000 - 2.000000\n",
      "weights(1,1,0): expected - actural 9.000000 - 9.000000\n",
      "weights(1,1,1): expected - actural 9.000000 - 9.000000\n",
      "weights(1,1,2): expected - actural 9.000000 - 9.000000\n",
      "weights(1,2,0): expected - actural 2.000000 - 2.000000\n",
      "weights(1,2,1): expected - actural 1.000000 - 1.000000\n",
      "weights(1,2,2): expected - actural 2.000000 - 2.000000\n",
      "weights(2,0,0): expected - actural 4.000000 - 4.000000\n",
      "weights(2,0,1): expected - actural 5.000000 - 5.000000\n",
      "weights(2,0,2): expected - actural 4.000000 - 4.000000\n",
      "weights(2,1,0): expected - actural 4.000000 - 4.000000\n",
      "weights(2,1,1): expected - actural 9.000000 - 9.000000\n",
      "weights(2,1,2): expected - actural 4.000000 - 4.000000\n",
      "weights(2,2,0): expected - actural 4.000000 - 4.000000\n",
      "weights(2,2,1): expected - actural 5.000000 - 5.000000\n",
      "weights(2,2,2): expected - actural 4.000000 - 4.000000\n",
      "input array:\n",
      "[[[1. 1. 2. 4.]\n",
      "  [5. 6. 7. 8.]\n",
      "  [3. 2. 1. 0.]\n",
      "  [1. 2. 3. 4.]]\n",
      "\n",
      " [[0. 1. 2. 3.]\n",
      "  [4. 5. 6. 7.]\n",
      "  [8. 9. 0. 1.]\n",
      "  [3. 4. 5. 6.]]]\n",
      "output array:\n",
      "[[[6. 8.]\n",
      "  [3. 4.]]\n",
      "\n",
      " [[5. 7.]\n",
      "  [9. 6.]]]\n",
      "input array:\n",
      "[[[1. 1. 2. 4.]\n",
      "  [5. 6. 7. 8.]\n",
      "  [3. 2. 1. 0.]\n",
      "  [1. 2. 3. 4.]]\n",
      "\n",
      " [[0. 1. 2. 3.]\n",
      "  [4. 5. 6. 7.]\n",
      "  [8. 9. 0. 1.]\n",
      "  [3. 4. 5. 6.]]]\n",
      "sensitivity array:\n",
      "[[[1. 2.]\n",
      "  [2. 4.]]\n",
      "\n",
      " [[3. 5.]\n",
      "  [8. 2.]]]\n",
      "delta array:\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 1. 0. 2.]\n",
      "  [2. 0. 0. 0.]\n",
      "  [0. 0. 0. 4.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 3. 0. 5.]\n",
      "  [0. 8. 0. 0.]\n",
      "  [0. 0. 0. 2.]]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%run activators.ipynb\n",
    "%run cnn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f9c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmLayer(object):\n",
    "    def __init__(self,input_width,state_width,learning_rate):\n",
    "        self.input_width = input_width\n",
    "        self.state_width = state_width\n",
    "        self.learning_rate = learning_rate\n",
    "         # 门的激活函数\n",
    "        self.gate_activator = SigmoidActivator()\n",
    "        # 输出的激活函数\n",
    "        self.output_activator = TanhActivator()\n",
    "        # 当前时刻初始化为t0\n",
    "        self.times = 0       \n",
    "        # 各个时刻的单元状态向量c\n",
    "        self.c_list = self.init_state_vec()\n",
    "        # 各个时刻的输出向量h\n",
    "        self.h_list = self.init_state_vec()\n",
    "        # 各个时刻的遗忘门f\n",
    "        self.f_list = self.init_state_vec()\n",
    "        # 各个时刻的输入门i\n",
    "        self.i_list = self.init_state_vec()\n",
    "        # 各个时刻的输出门o\n",
    "        self.o_list = self.init_state_vec()\n",
    "        # 各个时刻的即时状态c~\n",
    "        self.ct_list = self.init_state_vec()\n",
    "        # 遗忘门权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Wfh, self.Wfx, self.bf = (self.init_weight_mat())\n",
    "        # 输入门权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Wih, self.Wix, self.bi = (self.init_weight_mat())\n",
    "        # 输出门权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Woh, self.Wox, self.bo = (self.init_weight_mat())\n",
    "        # 单元状态权重矩阵Wfh, Wfx, 偏置项bf\n",
    "        self.Wch, self.Wcx, self.bc = (self.init_weight_mat())\n",
    "        \n",
    "    # 初始化保存状态的向量\n",
    "    def init_state_vec(self):\n",
    "        state_vec_list = []\n",
    "        state_vec_list.append(np.zeros((self.state_width, 1)))\n",
    "        return state_vec_list\n",
    "    \n",
    "    # 初始化权重矩阵\n",
    "    def init_weight_mat(self):\n",
    "        Wh = np.random.uniform(-1e-4, 1e-4,(self.state_width, self.state_width))\n",
    "        Wx = np.random.uniform(-1e-4, 1e-4,(self.state_width, self.input_width))\n",
    "        b = np.zeros((self.state_width, 1))\n",
    "        return Wh, Wx, b\n",
    "    \n",
    "    # 前向计算\n",
    "    def forward(self, x):\n",
    "        self.times += 1\n",
    "        # 遗忘门\n",
    "        fg = self.calc_gate(x, self.Wfx, self.Wfh, self.bf, self.gate_activator)\n",
    "        self.f_list.append(fg)\n",
    "        # 输入门\n",
    "        ig = self.calc_gate(x, self.Wix, self.Wih,self.bi, self.gate_activator)\n",
    "        self.i_list.append(ig)\n",
    "        # 输出门\n",
    "        og = self.calc_gate(x, self.Wox, self.Woh,self.bo, self.gate_activator)\n",
    "        self.o_list.append(og)\n",
    "        # 即时状态\n",
    "        ct = self.calc_gate(x, self.Wcx, self.Wch,self.bc, self.output_activator)\n",
    "        self.ct_list.append(ct)\n",
    "        # 单元状态\n",
    "        c = fg * self.c_list[self.times - 1] + ig * ct\n",
    "        self.c_list.append(c)\n",
    "        # 输出\n",
    "        h = og * self.output_activator.forward(c)\n",
    "        self.h_list.append(h)\n",
    "        \n",
    "    # 计算门\n",
    "    def calc_gate(self,x,Wx,Wh,b,activator):\n",
    "        h=self.h_list[self.times-1] # 上次的LSTM输出\n",
    "        net = np.dot(Wh,h)+np.dot(Wx,x)+b\n",
    "        gate=activator.forward(net)\n",
    "        return gate\n",
    "    \n",
    "    # LSTM训练算法\n",
    "    def backward(self,x,delta_h,activator):\n",
    "        self.calc_delta(delta_h,activator)\n",
    "        self.calc_gradient(x)\n",
    "        \n",
    "    # 按照梯度下降，更新权重\n",
    "    def update(self):\n",
    "        self.Wfh -= self.learning_rate * self.Whf_grad\n",
    "        self.Wfx -= self.learning_rate * self.Whx_grad\n",
    "        self.bf -= self.learning_rate * self.bf_grad\n",
    "        self.Wih -= self.learning_rate * self.Whi_grad\n",
    "        self.Wix -= self.learning_rate * self.Whi_grad\n",
    "        self.bi -= self.learning_rate * self.bi_grad\n",
    "        self.Woh -= self.learning_rate * self.Wof_grad\n",
    "        self.Wox -= self.learning_rate * self.Wox_grad\n",
    "        self.bo -= self.learning_rate * self.bo_grad\n",
    "        self.Wch -= self.learning_rate * self.Wcf_grad\n",
    "        self.Wcx -= self.learning_rate * self.Wcx_grad\n",
    "        self.bc -= self.learning_rate * self.bc_grad\n",
    "        \n",
    "    def calc_delta(self,delta_h,activator):\n",
    "        # 初始化各个时刻的误差项\n",
    "        self.delta_h_list = self.init_delta()  # 输出误差项\n",
    "        self.delta_o_list = self.init_delta()  # 输出门误差项\n",
    "        self.delta_i_list = self.init_delta()  # 输入门误差项\n",
    "        self.delta_f_list = self.init_delta()  # 遗忘门误差项\n",
    "        self.delta_ct_list = self.init_delta() # 即时输出误差项\n",
    "\n",
    "        # 保存从上一层传递下来的当前时刻的误差项\n",
    "        self.delta_h_list[-1] = delta_h\n",
    "        \n",
    "        # 迭代计算每个时刻的误差项\n",
    "        for k in range(self.times, 0, -1):\n",
    "            self.calc_delta_k(k)\n",
    "    \n",
    "    # 初始化误差项\n",
    "    def init_delta(self):\n",
    "        delta_list=[]\n",
    "        for i in range(self.time+1):\n",
    "            delta_list.append(np.zeros((self.state_width,1)))\n",
    "        return delta_list\n",
    "    \n",
    "    # 根据k时刻的delta_h，计算k时刻的delta_f、delta_i、delta_o、delta_ct，以及k-1时刻的delta_h\n",
    "    def calc_delta_k(self,k):\n",
    "        # 获得k时刻前向计算的值\n",
    "        ig = self.i_list[k]\n",
    "        og = self.o_list[k]\n",
    "        fg = self.f_list[k]\n",
    "        ct = self.ct_list[k]\n",
    "        c = self.c_list[k]\n",
    "        c_prev = self.c_list[k-1]\n",
    "        tanh_c = self.output_activator.forward(c)\n",
    "        delta_k = self.delta_h_list[k]\n",
    "        \n",
    "        delta_o = (delta_k * tanh_c * self.gate_activator.backward(og))\n",
    "        delta_f = (delta_k * og * (1 - tanh_c * tanh_c) * c_prev * self.gate_activator.backward(fg))\n",
    "        delta_i = (delta_k * og * (1 - tanh_c * tanh_c) * ct * self.gate_activator.backward(ig))\n",
    "        delta_ct = (delta_k * og * (1 - tanh_c * tanh_c) * ig * self.output_activator.backward(ct))\n",
    "        delta_h_prev = (\n",
    "                np.dot(delta_o.transpose(), self.Woh) +\n",
    "                np.dot(delta_i.transpose(), self.Wih) +\n",
    "                np.dot(delta_f.transpose(), self.Wfh) +\n",
    "                np.dot(delta_ct.transpose(), self.Wch)\n",
    "            ).transpose()\n",
    "\n",
    "        # 保存全部delta值\n",
    "        self.delta_h_list[k-1] = delta_h_prev\n",
    "        self.delta_f_list[k] = delta_f\n",
    "        self.delta_i_list[k] = delta_i\n",
    "        self.delta_o_list[k] = delta_o\n",
    "        self.delta_ct_list[k] = delta_ct\n",
    "        \n",
    "    def calc_gradient(self, x):\n",
    "        # 初始化遗忘门权重梯度矩阵和偏置项\n",
    "        self.Wfh_grad, self.Wfx_grad, self.bf_grad = (self.init_weight_gradient_mat())\n",
    "        # 初始化输入门权重梯度矩阵和偏置项\n",
    "        self.Wih_grad, self.Wix_grad, self.bi_grad = (self.init_weight_gradient_mat())\n",
    "        # 初始化输出门权重梯度矩阵和偏置项\n",
    "        self.Woh_grad, self.Wox_grad, self.bo_grad = (self.init_weight_gradient_mat())\n",
    "        # 初始化单元状态权重梯度矩阵和偏置项\n",
    "        self.Wch_grad, self.Wcx_grad, self.bc_grad = (self.init_weight_gradient_mat())\n",
    "        \n",
    "         # 计算对上一次输出h和偏置项b的权重梯度\n",
    "        for t in range(self.times, 0, -1):\n",
    "            # 计算各个时刻的梯度\n",
    "            (Wfh_grad, bf_grad,\n",
    "            Wih_grad, bi_grad,\n",
    "            Woh_grad, bo_grad,\n",
    "            Wch_grad, bc_grad) = (self.calc_gradient_t(t))\n",
    "            # 实际梯度是各时刻梯度之和\n",
    "            self.Wfh_grad += Wfh_grad\n",
    "            self.bf_grad += bf_grad\n",
    "            self.Wih_grad += Wih_grad\n",
    "            self.bi_grad += bi_grad\n",
    "            self.Woh_grad += Woh_grad\n",
    "            self.bo_grad += bo_grad\n",
    "            self.Wch_grad += Wch_grad\n",
    "            self.bc_grad += bc_grad\n",
    "            \n",
    "        # 计算对本次输入x的权重梯度\n",
    "        xt = x.transpose()\n",
    "        self.Wfx_grad = np.dot(self.delta_f_list[-1], xt)\n",
    "        self.Wix_grad = np.dot(self.delta_i_list[-1], xt)\n",
    "        self.Wox_grad = np.dot(self.delta_o_list[-1], xt)\n",
    "        self.Wcx_grad = np.dot(self.delta_ct_list[-1], xt)\n",
    "    \n",
    "    # 初始化权重矩阵\n",
    "    def init_weight_gradient_mat(self):\n",
    "        Wh_grad = np.zeros((self.state_width,self.state_width))\n",
    "        Wx_grad = np.zeros((self.state_width,self.input_width))\n",
    "        b_grad = np.zeros((self.state_width, 1))\n",
    "        return Wh_grad, Wx_grad, b_grad\n",
    "    \n",
    "    # 计算每个时刻t权重的梯度\n",
    "    def calc_gradient_t(self, t):\n",
    "        h_prev = self.h_list[t-1].transpose()\n",
    "        Wfh_grad = np.dot(self.delta_f_list[t], h_prev)\n",
    "        bf_grad = self.delta_f_list[t]\n",
    "        Wih_grad = np.dot(self.delta_i_list[t], h_prev)\n",
    "        bi_grad = self.delta_f_list[t]\n",
    "        Woh_grad = np.dot(self.delta_o_list[t], h_prev)\n",
    "        bo_grad = self.delta_f_list[t]\n",
    "        Wch_grad = np.dot(self.delta_ct_list[t], h_prev)\n",
    "        bc_grad = self.delta_ct_list[t]\n",
    "        return Wfh_grad, bf_grad, Wih_grad, bi_grad, Woh_grad, bo_grad, Wch_grad, bc_grad\n",
    "    \n",
    "    def reset_state(self):\n",
    "        # 当前时刻初始化为t0\n",
    "        self.times = 0       \n",
    "        # 各个时刻的单元状态向量c\n",
    "        self.c_list = self.init_state_vec()\n",
    "        # 各个时刻的输出向量h\n",
    "        self.h_list = self.init_state_vec()\n",
    "        # 各个时刻的遗忘门f\n",
    "        self.f_list = self.init_state_vec()\n",
    "        # 各个时刻的输入门i\n",
    "        self.i_list = self.init_state_vec()\n",
    "        # 各个时刻的输出门o\n",
    "        self.o_list = self.init_state_vec()\n",
    "        # 各个时刻的即时状态c~\n",
    "        self.ct_list = self.init_state_vec()\n",
    "\n",
    "def data_set():\n",
    "    x = [np.array([[1], [2], [3]]),\n",
    "         np.array([[2], [3], [4]])]\n",
    "    d = np.array([[1], [2]])\n",
    "    return x, d\n",
    "\n",
    "# 梯度检查\n",
    "def gradient_check():\n",
    "    # 设计一个误差函数，取所有节点输出项之和\n",
    "    error_function = lambda o: o.sum()\n",
    "    \n",
    "    lstm = LstmLayer(3, 2, 1e-3)\n",
    "\n",
    "    # 计算forward值\n",
    "    x, d = data_set()\n",
    "    lstm.forward(x[0])\n",
    "    lstm.forward(x[1])\n",
    "    \n",
    "    # 求取sensitivity map\n",
    "    sensitivity_array = np.ones(lstm.h_list[-1].shape,dtype=np.float64)\n",
    "    # 计算梯度\n",
    "    lstm.backward(x[1], sensitivity_array, IdentityActivator())\n",
    "    \n",
    "    # 检查梯度\n",
    "    epsilon = 10e-4\n",
    "    for i in range(lstm.Wfh.shape[0]):\n",
    "        for j in range(lstm.Wfh.shape[1]):\n",
    "            lstm.Wfh[i,j] += epsilon\n",
    "            lstm.reset_state()\n",
    "            lstm.forward(x[0])\n",
    "            lstm.forward(x[1])\n",
    "            err1 = error_function(lstm.h_list[-1])\n",
    "            lstm.Wfh[i,j] -= 2*epsilon\n",
    "            lstm.reset_state()\n",
    "            lstm.forward(x[0])\n",
    "            lstm.forward(x[1])\n",
    "            err2 = error_function(lstm.h_list[-1])\n",
    "            expect_grad = (err1 - err2) / (2 * epsilon)\n",
    "            lstm.Wfh[i,j] += epsilon\n",
    "            print('weights(%d,%d): expected - actural %.4e - %.4e' % (i, j, expect_grad, lstm.Wfh_grad[i,j]))\n",
    "    return lstm\n",
    "\n",
    "def test():\n",
    "    l = LstmLayer(3, 2, 1e-3)\n",
    "    x, d = data_set()\n",
    "    l.forward(x[0])\n",
    "    l.forward(x[1])\n",
    "    l.backward(x[1], d, IdentityActivator())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f366991",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LstmLayer' object has no attribute 'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     test()\n\u001b[1;32m      3\u001b[0m     gradient_check()\n",
      "Cell \u001b[0;32mIn[4], line 264\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m l\u001b[38;5;241m.\u001b[39mforward(x[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    263\u001b[0m l\u001b[38;5;241m.\u001b[39mforward(x[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 264\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward(x[\u001b[38;5;241m1\u001b[39m], d, IdentityActivator())\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m l\n",
      "Cell \u001b[0;32mIn[4], line 77\u001b[0m, in \u001b[0;36mLstmLayer.backward\u001b[0;34m(self, x, delta_h, activator)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,delta_h,activator):\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_delta(delta_h,activator)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_gradient(x)\n",
      "Cell \u001b[0;32mIn[4], line 97\u001b[0m, in \u001b[0;36mLstmLayer.calc_delta\u001b[0;34m(self, delta_h, activator)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_delta\u001b[39m(\u001b[38;5;28mself\u001b[39m,delta_h,activator):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# 初始化各个时刻的误差项\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_h_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_delta()  \u001b[38;5;66;03m# 输出误差项\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_o_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_delta()  \u001b[38;5;66;03m# 输出门误差项\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_i_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_delta()  \u001b[38;5;66;03m# 输入门误差项\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 113\u001b[0m, in \u001b[0;36mLstmLayer.init_delta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_delta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    112\u001b[0m     delta_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    114\u001b[0m         delta_list\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_width,\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m delta_list\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LstmLayer' object has no attribute 'time'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test()\n",
    "    gradient_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd106f7",
   "metadata": {},
   "source": [
    "为什么初始化误差时，delta_list的times要加一，而初始化梯度时不需要?\n",
    "    在循环神经网络（RNN）和特别是LSTM（长短期记忆）网络中，delta_list 通常用于存储每个时间步的隐藏状态 h 和细胞状态 c 的梯度（或称为敏感度）。在初始化这些梯度时，delta_list 的长度通常与输入序列的长度（时间步的数量）相同。\n",
    "    当我们谈论初始化误差时，delta_list 的长度加一的原因通常与网络的输出层有关。在序列到序列（seq2seq）模型或带有输出层的RNN中，最后一个时间步的输出通常用于计算损失函数（如交叉熵损失）。在计算损失函数关于模型参数的梯度时，我们需要考虑输出层的贡献。\n",
    "    因此，delta_list 的最后一个元素（对应于输出层的梯度）是在损失函数计算后单独计算的，而不是通过时间反向传播（Backpropagation Through Time, BPTT）得到的。\n",
    "    然而，当我们初始化梯度时，我们是在为BPTT做准备，所以我们只关心隐藏状态和细胞状态的梯度，这些梯度是通过时间步反向传播的。因此，初始化梯度时，我们不需要为输出层额外添加一个元素。\n",
    "简而言之：\n",
    "    初始化误差（delta_list）时，我们需要为输出层的梯度额外添加一个元素，因此长度加一。\n",
    "    初始化梯度时，我们只关心隐藏状态和细胞状态的梯度，所以不需要额外添加元素。\n",
    "    需要注意的是，具体的实现细节可能会根据网络架构和所使用的库而有所不同。在一些实现中，可能不需要显式地初始化 delta_list，因为库会自动处理这些计算。然而，了解这些基本概念对于理解RNN和LSTM的训练过程是非常重要的。\n",
    "\n",
    "为什么delta_h的初始值为sensitivity_array = np.ones(lstm.h_list[-1].shape, dtype=np.float64)?\n",
    "    在LSTM（长短期记忆）网络中，delta_h 通常指的是隐藏状态 h 的梯度（或敏感度），这些梯度在反向传播过程中用于更新网络的权重。初始化 delta_h 为一个与隐藏状态形状相同的全1数组 sensitivity_array 的原因通常与反向传播算法和梯度计算有关。\n",
    "    首先，让我们回顾一下反向传播算法的基本思想:\n",
    "    在反向传播过程中，我们需要计算损失函数对于网络参数的梯度，以便使用这些梯度来更新参数并减少模型的预测错误。对于循环神经网络（RNNs）和特别是LSTM，这涉及到计算损失函数对于每个时间步的隐藏状态和细胞状态的梯度。在LSTM中，每个时间步的隐藏状态 h 和细胞状态 c 是根据前一个时间步的隐藏状态和输入来计算的。因此，在计算梯度时，我们需要知道每个时间步的隐藏状态和细胞状态如何影响最终的损失。\n",
    "    初始化 delta_h 为全1数组的原因是为了在反向传播开始时为所有隐藏状态提供相同的初始敏感度。这意味着一开始时，我们假设每个隐藏状态对损失函数的影响都是相同的。随着反向传播的进行，delta_h 的值会根据链式法则和网络的权重进行更新，以反映每个隐藏状态对最终损失的实际影响。\n",
    "    此外，使用全1数组作为初始值可以简化计算，因为所有的隐藏状态在开始时都有相同的敏感度，这有助于避免在计算梯度时引入额外的偏差。随着反向传播的进行，这些敏感度会根据网络的实际结构和权重进行调整。\n",
    "    总的来说，将 delta_h 初始化为全1数组是反向传播算法的一部分，用于在开始时为所有隐藏状态提供相同的敏感度，并在后续的计算中根据网络的实际情况进行更新。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
